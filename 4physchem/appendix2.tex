Рассмотрим интеграл некоторой функции $n$ переменных $\mf{x} = \lb x_1, \dots, x_n \rb$ по некоторой области $\Omega \subset \mathbb{R}^n$:
\vverh
\begin{gather}
I = \int\limits_{\Omega} f \lb \mf{x} \rb d \mf{x} \label{mint1} 
\end{gather}

Будем оценивать интеграл \eqref{mint1}, задавая некоторое случайное распределение $M$ точек по $\Omega$ с плотностью вероятности $\rho(\mf{x})$:
\vverh
\begin{gather}
S^{\lb 1 \rb} = \frac{1}{M} \sum_{\mf{x}} \frac{f \lb \mf{x} \rb}{\rho \lb \mf{x} \rb} \, \longrightarrow I, \text{при} \, M \longrightarrow \infty. \notag
\end{gather}

Построенная оценка интеграла $S^{\lb 1 \rb}$ также является случайной величиной, её дисперсия равна:
\vverh
\begin{gather}
\sigma^2 = \frac{1}{M} \left[ \int\limits_{\Omega} \frac{f^2 \lb \mf{x} \rb}{\rho \lb \mf{x} \rb} d \mf{x} - \lb \int\limits_{\Omega} f \lb \mf{x} \rb d \mf{x} \rb^2 \, \right] \notag
\end{gather}

При больших $M$ дисперсия аппроксимируется следующим выражением:
\vverh
\begin{gather}
\sigma^2 \simeq \frac{1}{M - 1} \lb S^{\lb 2 \rb} - \lb S^{\lb 1 \rb} \rb^2 \rb, \quad S^{\lb 2 \rb} = \frac{1}{M} \sum_{\mf{x}} \lb \frac{f \lb \mf{x} \rb}{\rho \lb \mf{x} \rb} \rb^2. \notag
\end{gather}

Среднеквадратичное отклонение $\sigma$ показывает, насколько точно величина $S^{\lb 1 \rb}$ оценивает интеграл $I$ \eqref{mint1}. Существует несколько техник уменьшения СКО $\sigma$ при фиксированном $M$:
\begin{itemize}
\item Выборка по значимости (\textit{Importance sampling}). Идея метода заключается в том, что некоторые значения случайной величины имеют б\'{o}льшую значимость (вероятность) для оцениваемой функции, чем другие. Если более значимые значения будут вносить больший вес в оценку интеграла, то дисперсия уменьшится. Несложно убедиться, что оптимальное распределение $\rho \lb \mf{x} \rb$  есть
\vverh
\begin{gather}
\rho \lb \mf{x} \rb = \ddfrac{| f(\mf{x}) |}{\int\limits_{\Omega} |f(\mf{x})| d \mf{x}} \notag
\end{gather} 
Иными словами, идея метода заключается в "концентрировании"\ плотности $\rho \lb \mf{x} \rb$ в тех областях $\Omega$, где $| f \lb \mf{x} \rb |$ максимально. 
\item Стратифицированная выборка (\textit{Stratified sampling}). Идея этого метода заключается в разбиении объема $\Omega$ на $N$ более маленьких объемов разных размеров. Интегрирование методом Монте-Карло выполняется в каждом из маленьких объемов с использованием $\frac{M}{N}$ точек. Изменяя относительный размер и расположение маленьких объемов, мы изменяем их вклад как в значение интеграла, так и в дисперсию. Общее значение дисперсии становится минимальным, когда вклад каждого объема одинаков и равен $\frac{\sigma^2}{N}$.  
\end{itemize} 

В чистом виде эти методики не применимы для general purpose алгоритмов интегрирования (т.е. для таких алгоритмов, которые не используют априорного знания о поведении подынтегральной функции), однако разработаны итеративные алгоритмы, позволяющие использовать информацию о подынтегральной функции, полученной в ходе одной итерации, для улучшения оценки в следующей. \cite{lepage1978, tsuda1973, haselgrove1961}
